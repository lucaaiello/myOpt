---
title: "myOpt package to fit linear models"
author: "Aiello Luca, Piacenza Fabio"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{myOpt_usage}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(rmarkdown.html_vignette.check_title = FALSE)
```

## 1. Introduction

The goal of myOpt package is to provide users an efficient way to estimate parameters of a linear model through optimization techniques. 

## 2. Gradient descent method

In the following lines, the results and the computation times are compared,  using the function of this package for gradient descent method ("linear_gd_optim") and the standard one ("lm").

```{r setup}
library(myOpt)

## basic example code

set.seed(8675309)

# data simulation for example purposes

n = 1000

x1 = rnorm(n)
x2 = rnorm(n)
X <- cbind(rep(1,n),x1,x2) # predictor matrix

Y = 5.6 + 2.3*x1 + 8.7*x2 + rnorm(n) # response vector

# random initial values for the parameters of the linear model

par <- rnorm(dim(X)[2])

# the function returns a vector containing the values of the estimated parameters

opt_par <- linear_gd_optim(par, X, Y)

# standard lm function for estimating the parameters

lm_par <- lm(Y ~ x1 + x2)$coefficients

opt_par

lm_par

## Timing comparison between functions

library(tictoc)

tic()
linear_gd_optim(par, X, Y)
toc()

tic()
linear_gd_optim_old(par, X, Y)
toc()

tic()
lm(Y ~ x1 + x2)
toc()
```

As it is possible to notice, the results are quite the same, meaning that the accuracy of the method implemented is, in terms of results, comparable to the standard one. 
The vectorized version of the optimization function is much faster than the one using for loops, and the standard function is the fastest one. 
Now let's benchmark the vectorized version of the the function "linear_gd_optim" versus the one using only for loops.

```{r, fig.show="hold", fig.align = "center", fig.width = 7, fig.asp = .62}

library(ggplot2)
library(dplyr)
library(tidyr)
library(bench)

## Useful functions

# Plot a benchmark
show_bm <- function(bm) {
  print(print_bench(bm))
  autoplot(bm)
}

# printable bench (for RMarkdown)
print_bench <- function(bm) {
  bm %>% 
    mutate(expression = as.character(expression))
}


## Benchmarks 

bench::mark(
  vec_method = linear_gd_optim(par, X, Y),
  for_method = linear_gd_optim_old(par, X, Y),
  filter_gc = FALSE,
) %>%
  show_bm()
```

From the benchmarks it is possible to see how the vectorized version of the function performs much faster than the other one; this allows our function to compete with the standard one "lm" not only in terms of result accuracy but also in terms of computation times.


## 3. Steepest descent method

The parameters of the linear model can also be calculated with the steepest descend method, using the function "linear_sd_optim" implemented in this package.

The example of the previous section is repeated applying the steepest descend method, comparing results and computation times with the gradient descent method and the standard function "lm".

```{r setup sd}
library(myOpt)

## basic example code

set.seed(8675309)

# data simulation for example purposes

n = 1000

x1 = rnorm(n)
x2 = rnorm(n)
X <- cbind(rep(1,n),x1,x2) # predictor matrix

Y = 5.6 + 2.3*x1 + 8.7*x2 + rnorm(n) # response vector

# random initial values for the parameters of the linear model

par <- rnorm(dim(X)[2])

# the function returns a vector containing the values of the estimated parameters

opt_par <- linear_sd_optim(par, X, Y)

# standard lm function for estimating the parameters

lm_par <- lm(Y ~ x1 + x2)$coefficients

opt_par

lm_par

## Timing comparison between functions

library(tictoc)

tic()
linear_gd_optim(par, X, Y)
toc()

tic()
linear_sd_optim(par, X, Y)
toc()

tic()
lm(Y ~ x1 + x2)
toc()
```

It can be noted that the results are quite similar, meaning that also the accuracy of the implemented steepest descent is, in terms of results, comparable to the standard function "lm". 
The vectorized version of the gradient descent method is faster than the steepest descent one, while the standard function remains the fastest one. 
We proceed benchmarking the functions "linear_gd_optim" (based on the gradient descent) and "linear_sd_optim" (based on the steepest descent).

```{r, fig.show="hold", fig.align = "center", fig.width = 7, fig.asp = .62}

library(ggplot2)
library(dplyr)
library(tidyr)
library(bench)

## Useful functions

# Plot a benchmark
show_bm <- function(bm) {
  print(print_bench(bm)) 
  autoplot(bm)
}

# printable bench (for RMarkdown)
print_bench <- function(bm) {
  bm %>% 
    mutate(expression = as.character(expression))
}


## Benchmarks 

bench::mark(
  gd_method = round(linear_gd_optim(par, X, Y),1),
  sd_method = round(linear_sd_optim(par, X, Y),1),
  filter_gc = FALSE,
) %>%
  show_bm()

```

From the benchmarks it is again possible to observe that the gradient descent method is faster than the steepest descent one, in front of a comparable accuracy.
This is essentially due to the additional operations that the steepest descent has to perform at each iteration of the optimization procedure to calculate the step, compared to the gradient method.


## 4. Gradient and steepest descent methods using more predictors

This section shows that the gradient and steepest descent methods, implemented in this package, work using even more than two predictors. 

A new example, including three predictors, is reported in this section section applying the gradient and the steepest descend methods, comparing results and computation times with the standard function "lm".

```{r setup three predictors}
library(myOpt)

## basic example code

set.seed(8675309)

# data simulation for example purposes

n = 1000

x1 = rnorm(n)
x2 = rnorm(n)
x3 = rnorm(n)
X <- cbind(rep(1,n),x1,x2,x3) # predictor matrix

Y = 5.6 + 2.3*x1 + 8.7*x2 + 7.2*x3 + rnorm(n) # response vector

# random initial values for the parameters of the linear model

par <- rnorm(dim(X)[2])

# the function returns a vector containing the values of the estimated parameters

opt_par_gd <- linear_gd_optim(par, X, Y)
opt_par_sd <- linear_sd_optim(par, X, Y)

# standard lm function for estimating the parameters

lm_par <- lm(Y ~ x1 + x2 + x3)$coefficients

opt_par_gd

opt_par_sd

lm_par

## Timing comparison between functions

library(tictoc)

tic()
linear_gd_optim(par, X, Y)
toc()

tic()
linear_sd_optim(par, X, Y)
toc()

tic()
lm(Y ~ x1 + x2 + x3)
toc()
```

It can be noted that the results are quite similar, meaning that also the accuracy of the implemented steepest gradient and steepest descent methods are, in terms of results, comparable to the standard function "lm", also using three predictors. 
The vectorized version of the gradient descent method is still faster than the steepest descent one, while the standard function remains the fastest one. 
We proceed benchmarking the functions "linear_gd_optim" (based on the gradient descent) and "linear_sd_optim" (based on the steepest descent) using three predictors.

```{r, fig.show="hold", fig.align = "center", fig.width = 7, fig.asp = .62}

library(ggplot2)
library(dplyr)
library(tidyr)

## Useful functions

# Plot a benchmark
show_bm <- function(bm) {
  print(print_bench(bm)) 
  autoplot(bm)
}

show_bm <- function(bm) {
  print(print_bench(bm)) 
  autoplot(bm)
}


# printable bench (for RMarkdown)
print_bench <- function(bm) {
  bm %>% 
    mutate(expression = as.character(expression))
}


## Benchmarks 

bench::mark(
  gd_method = round(linear_gd_optim(par, X, Y),1),
  sd_method = round(linear_sd_optim(par, X, Y),1),
  filter_gc = FALSE,
) %>%
  show_bm()

```

The benchmarks confirm that the gradient descent method is faster than the steepest descent one, in front of a comparable accuracy, also using three predictors.


## 5. Prediction

After the estimation of the parameters, for both method it can be applied a prediction function, that predicts the outcome given the estimated parameters vector of the model and a predictors data matrix.

```{r}
# prediction with the parameters estimated through the lm function
prediction_lm <- predict(lm(Y ~ x1 + x2 + x3))

# prediction with the parameters estimated through the gradient descend method
prediction_gd <- my_linear_predict(opt_par_gd, X)

# prediction with the parameters estimated through the steepest descend method
prediction_sd <- my_linear_predict(opt_par_sd, X)
```

After the computation of the predictions this package allows also to compute the error associated with the prediction.

```{r}
# prediction error associated to the lm function estimated parameters
my_pred_error(Y, prediction_lm)

# prediction error associated to the gradient descend estimated parameters
my_pred_error(Y, prediction_gd)

# prediction error associated to the steepest descend estimated parameters
my_pred_error(Y, prediction_sd)

```

It is possible to see how the three methods give similar results in terms of prediction and also in terms of prediction error.


## 6. Conclusions

This document shows how to apply the functions to estimate the linear regression parameters, implemented in this package, based on the gradient descent and steepest descent methods.

Several examples are reported, including comparisons and benchmarks between the two implemented methods, and versus the standard function "lm".
All the provided results agree on the fact that the implemented methods are comparable, in terms of accuracy, with respect to the standard function. 

In terms of computation time, gradient descent method is faster than the steepest descent one, because of the simpler step calculation at each iteration.
We can observe that, even if it is a bit slower, the gradient descent method reaches computation times which are comparable to standard function. 





